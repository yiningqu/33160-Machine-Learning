{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "For each of the shap_features in the files in the folders Random Forest Parameters and Gradient Boosting Parameters, find the 10 most common features and use those as the optimal features.\n",
    "\n",
    "Find the P/L curves and compute annualized Sharp Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: dask in /opt/anaconda3/lib/python3.8/site-packages (2023.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: click>=8.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from dask) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask) (3.11.0)\n",
      "Requirement already satisfied: locket in /opt/anaconda3/lib/python3.8/site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.2.2.\n"
     ]
    }
   ],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_pickle('/Users/yiningqu/Desktop/研究生/33160 Machine Learning/Assignment2/dataset.pkl')\n",
    "data = raw_data[raw_data['market_cap'] > 1000.0]\n",
    "data = data.copy()\n",
    "data.fillna(0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x > 0.01:\n",
    "        return 1\n",
    "    elif x < -0.025:\n",
    "        return -1\n",
    "    else:\n",
    "    \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rel_performance'] = data['pred_rel_return'].apply(f)\n",
    "data.reset_index(inplace=True)\n",
    "data.set_index('date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = [pd.to_datetime('2001-01-01') + pd.DateOffset(months = 3*i) for i in range(57)]\n",
    "end_dates = [d + pd.DateOffset(months = 36) for d in start_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames = [data.loc[d:d+pd.DateOffset(months = 36)] for d in start_dates]\n",
    "valid_frames = [data.loc[d:d+pd.DateOffset(months = 3)] for d in end_dates]\n",
    "test_frames = [data.loc[d+pd.DateOffset(months = 6):d+pd.DateOffset(months = 9)] for d in end_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [d.reset_index().drop\n",
    "                                 (['ticker','date',\n",
    "                                   'next_period_return',\n",
    "                                   'spy_next_period_return',\n",
    "                                   'rel_performance','pred_rel_return',\n",
    "                                  'return', 'cum_ret', 'spy_cum_ret'],axis=1) for d in training_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = [d.reset_index().drop\n",
    "                                 (['ticker','date',\n",
    "                                   'next_period_return',\n",
    "                                   'spy_next_period_return',\n",
    "                                   'rel_performance','pred_rel_return',\n",
    "                                  'return', 'cum_ret', 'spy_cum_ret'],axis=1) for d in valid_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [d.reset_index().drop(['ticker','date',\n",
    "                                   'next_period_return',\n",
    "                                   'spy_next_period_return',\n",
    "                                   'rel_performance','pred_rel_return',\n",
    "                                  'return', 'cum_ret', 'spy_cum_ret'],axis=1) for d in test_frames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = [d['rel_performance'].values for d in training_frames]\n",
    "valid_labels = [d['rel_performance'].values for d in valid_frames] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(start_dates)-1):\n",
    "    float_vars = [x for x in training_data[i].columns if data[x].dtype == 'float64']\n",
    "    scaler = StandardScaler()\n",
    "    training_data[i] = training_data[i].copy()\n",
    "    valid_data[i] = valid_data[i].copy()\n",
    "    test_data[i] = test_data[i].copy()\n",
    "    training_data[i][float_vars] = scaler.fit_transform(training_data[i][float_vars])\n",
    "    valid_data[i][float_vars] = scaler.transform(valid_data[i][float_vars])\n",
    "    test_data[i][float_vars] = scaler.transform(test_data[i][float_vars])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial:Trial,train=None,labels=None,val=None,val_labels=None,val_rets=None):\n",
    "\n",
    "    rf_n_estimators = trial.suggest_int('n_estimators', 10,40,step=5)\n",
    "    rf_max_features = trial.suggest_categorical('max_features',['sqrt','log2'])\n",
    "    rf_min_samples_leaf = trial.suggest_int('min_samples_leaf',400,2400,step=400)\n",
    "    rf_max_depth = trial.suggest_int('max_depth',5,25,step=5)\n",
    "    \n",
    "    rf_clf = RandomForestClassifier(n_estimators=rf_n_estimators,\n",
    "                                    max_depth=rf_max_depth,\n",
    "                                    min_samples_leaf=rf_min_samples_leaf,\n",
    "                                    max_features=rf_max_features)\n",
    "    rf_clf.fit(train,labels)\n",
    "    preds = rf_clf.predict(val)\n",
    "    profit = (preds * val_rets).sum()\n",
    " \n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_hyperparameters_rf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 10, 'max_features': 'log2', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 25, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1200, 'max_depth': 10}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 15}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1200, 'max_depth': 20}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 5}\n",
      "{'n_estimators': 25, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 2000, 'max_depth': 10}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 25, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 20}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1200, 'max_depth': 5}\n",
      "{'n_estimators': 30, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 5}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 5}\n",
      "{'n_estimators': 30, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 15}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 15}\n",
      "{'n_estimators': 25, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 25, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2000, 'max_depth': 15}\n",
      "{'n_estimators': 25, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 25}\n",
      "{'n_estimators': 25, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 15}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 2400, 'max_depth': 15}\n",
      "{'n_estimators': 10, 'max_features': 'log2', 'min_samples_leaf': 1200, 'max_depth': 10}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 15}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 5}\n",
      "{'n_estimators': 10, 'max_features': 'log2', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 15}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 2000, 'max_depth': 20}\n",
      "{'n_estimators': 10, 'max_features': 'log2', 'min_samples_leaf': 400, 'max_depth': 5}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 10}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 2000, 'max_depth': 5}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 20}\n",
      "{'n_estimators': 20, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 20}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1600, 'max_depth': 10}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 10, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 20}\n",
      "{'n_estimators': 35, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 25}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 30, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 25}\n",
      "{'n_estimators': 25, 'max_features': 'log2', 'min_samples_leaf': 800, 'max_depth': 15}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n",
      "{'n_estimators': 15, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 15, 'max_features': 'log2', 'min_samples_leaf': 2400, 'max_depth': 20}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2400, 'max_depth': 5}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 20}\n",
      "{'n_estimators': 40, 'max_features': 'sqrt', 'min_samples_leaf': 2000, 'max_depth': 20}\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'min_samples_leaf': 800, 'max_depth': 15}\n",
      "{'n_estimators': 20, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 20}\n",
      "{'n_estimators': 35, 'max_features': 'sqrt', 'min_samples_leaf': 2000, 'max_depth': 25}\n",
      "{'n_estimators': 30, 'max_features': 'sqrt', 'min_samples_leaf': 400, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(start_dates)-1):\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    study.optimize(partial(objective,\n",
    "                           train=training_data[i],\n",
    "                           labels=training_labels[i],\n",
    "                           val=valid_data[i],\n",
    "                           val_labels=valid_labels,\n",
    "                           val_rets=valid_frames[i]['next_period_return']), n_trials=200,n_jobs=-1)\n",
    "    \n",
    "    optimal_hyperparameters_rf.append(study.best_params)  \n",
    "    print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_rf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp_par in optimal_hyperparameters_rf:\n",
    "    rf_clf = RandomForestClassifier(**hyp_par)\n",
    "    classifiers_rf.append(rf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/yiningqu/Desktop/研究生/33160 Machine Learning/Assignment4/shap_features.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     shap_features \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/Users/yiningqu/Desktop/研究生/33160 Machine Learning/Assignment4/shap_features.pkl\") as f:\n",
    "    shap_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top 10 most common features\n",
    "shap_features_rf = shap_features\n",
    "all_features_rf = [feature for sublist in shap_features_rf for feature in sublist]\n",
    "count_feature_rf = Counter(all_features_rf)\n",
    "optimal_feature_rf = count_feature_rf.most_common(10)\n",
    "optimal_feature_rf = [feature[0] for feature in optimal_feature_rf[:10]]\n",
    "optimal_feature_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_rf = [StandardScaler() for i in range(len(start_dates)-1)]  \n",
    "        \n",
    "opt_training_data = [pd.DataFrame(scalers_rf[i].fit_transform(training_frames[i][optimal_feature_rf].values),columns=optimal_feature_rf) for i in range(len(start_dates)-1)]\n",
    "opt_valid_data = [pd.DataFrame(scalers_rf[i].fit_transform(training_frames[i][optimal_feature_rf].values),columns=optimal_feature_rf) for i in range(len(start_dates)-1)]\n",
    "opt_test_data = [pd.DataFrame(scalers_rf[i].transform(test_frames[i][optimal_feature_rf].values),columns=optimal_feature_rf) for i in range(len(start_dates)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rf = [1]\n",
    "ret_rf = []\n",
    "\n",
    "for i in range(len(start_dates)-1):\n",
    "        \n",
    "        classifiers_rf[i].fit(opt_training_data[i],training_labels[i])\n",
    "\n",
    "        preds = classifiers_rf[i].predict(opt_test_data[i])\n",
    "        profit_i = (preds*test_frames[i]['next_period_return']).sum()\n",
    "        ret_rf.append(profit_i)\n",
    "        num_names = len(opt_test_data[i])\n",
    "        x_rf.append(x_rf[i] + (x_rf[i]/num_names)*profit_i)\n",
    "\n",
    "plt.plot(x_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = pd.read_pickle(r\"SPY_cum_ret.pkl\")\n",
    "SPY = SPY.loc['2004-07-01':'2018-09-30']\n",
    "SPY = SPY.resample('Q').ffill()\n",
    "SPY['spy_cum_ret'] = (SPY['spy_cum_ret'] - SPY['spy_cum_ret'][0] + 1)\n",
    "SPY['strategy'] = x_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = SPY.resample('Y').ffill()\n",
    "SPY.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_mean_ret = (SPY['strategy'] - 1).diff().mean()\n",
    "strategy_std = (SPY['strategy'] - 1).diff().std()\n",
    "strategy_sr = strategy_mean_ret/strategy_std\n",
    "print('Annualized Strategy Sharpe Ratio with Random Forest: ',strategy_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_2(trial:Trial,train=None,labels=None,val=None,val_labels=None,val_rets=None):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\":trial.suggest_int('n_estimators',10,100,step=10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 5, 25),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',400,2400,step=400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',0.01,0.2,step=0.01)\n",
    "    }\n",
    "    \n",
    "    gb_clf = LGBMClassifier(**params)\n",
    "    gbm = gb_clf.fit(train,labels)\n",
    "    preds = gbm.predict(val)\n",
    "    profit = (preds*val_rets).sum()\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_hyperparameters_gb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(start_dates)-1):\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    study.optimize(partial(objective_2,\n",
    "                           train=training_data[i],\n",
    "                           labels=training_labels[i],\n",
    "                           val=valid_data[i],\n",
    "                           val_labels=valid_labels[i],\n",
    "                           val_rets=valid_frames[i]['next_period_return']), n_trials=25,n_jobs=-1)\n",
    "    \n",
    "    optimal_hyperparameters_gb.append(study.best_params)  \n",
    "    print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_gb = []\n",
    "\n",
    "for hyp_par in optimal_hyperparameters_gb:\n",
    "    gb_clf = LGBMClassifier(**hyp_par)\n",
    "    classifiers_gb.append(gb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"shap_features_gb_clf_01.pkl\",'rb') as f:\n",
    "    shap_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top 10 most common features\n",
    "shap_features_gb = shap_features\n",
    "all_features_gb = [feature for sublist in shap_features_gb for feature in sublist]\n",
    "count_feature_gb = Counter(all_features_gb)\n",
    "optimal_feature_gb = count_feature_gb.most_common(10)\n",
    "optimal_feature_gb = [feature[0] for feature in optimal_feature_gb[:10]]\n",
    "optimal_feature_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_gb = [StandardScaler() for i in range(len(start_dates)-1)]  \n",
    "        \n",
    "opt_training_data = [pd.DataFrame(scalers_gb[i].fit_transform(training_frames[i][optimal_feature_gb].values),columns=optimal_feature_gb) for i in range(len(start_dates)-1)]\n",
    "opt_valid_data = [pd.DataFrame(scalers_gb[i].fit_transform(training_frames[i][optimal_feature_gb].values),columns=optimal_feature_gb) for i in range(len(start_dates)-1)]\n",
    "opt_test_data = [pd.DataFrame(scalers_gb[i].transform(test_frames[i][optimal_feature_gb].values),columns=optimal_feature_gb) for i in range(len(start_dates)-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gb = [1]\n",
    "ret_gb = []\n",
    "\n",
    "for i in range(len(start_dates)-1):\n",
    "        \n",
    "        classifiers_gb[i].fit(opt_training_data[i],training_labels[i])\n",
    "\n",
    "        preds = classifiers_gb[i].predict(opt_test_data[i])\n",
    "        profit_i = (preds*test_frames[i]['next_period_return']).sum()\n",
    "        ret_gb.append(profit_i)\n",
    "        num_names = len(opt_test_data[i])\n",
    "        x_gb.append(x_gb[i] + (x_gb[i]/num_names)*profit_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = pd.read_pickle(r\"SPY_cum_ret.pkl\")\n",
    "SPY = SPY.loc['2004-07-01':'2018-09-30']\n",
    "SPY = SPY.resample('Q').ffill()\n",
    "SPY['spy_cum_ret'] = (SPY['spy_cum_ret'] - SPY['spy_cum_ret'][0] + 1)\n",
    "SPY['strategy'] = x_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = SPY.resample('Y').ffill()\n",
    "SPY.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_mean_ret = (SPY['strategy'] - 1).diff().mean()\n",
    "strategy_std = (SPY['strategy'] - 1).diff().std()\n",
    "strategy_sr = strategy_mean_ret/strategy_std\n",
    "print('Annualized Strategy Sharpe Ratio with Gradient Boosting: ',strategy_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**:\n",
    "Combine these strategies, by choosing weights on each strategy and try to find a combined strategy with the best Sharp Ratio. You can also try to include a short of the SPY by including SPY with a negative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = pd.read_pickle(r\"SPY_cum_ret.pkl\")\n",
    "SPY = SPY.loc['2004-07-01':'2018-09-30']\n",
    "SPY = SPY.resample('Q').ffill()\n",
    "SPY['spy_cum_ret'] = (SPY['spy_cum_ret'] - SPY['spy_cum_ret'][0] + 1)\n",
    "SPY['strategy_rf'] = x_rf\n",
    "SPY['strategy_gb'] = x_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPY = SPY.resample('Y').ffill()\n",
    "SPY.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 50% weight of Random Forest and 50% weight of Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 50% weight of random forest strategy and 50% weight of gradient boosting\n",
    "SPY['rf50gb50'] = SPY['strategy_rf']*0.5+SPY['strategy_gb']*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf50gb50_mean_ret = (SPY['rf50gb50'] - 1).diff().mean()\n",
    "rf50gb50_std = (SPY['rf50gb50'] - 1).diff().std()\n",
    "rf50gb50_sr = rf50gb50_mean_ret/rf50gb50_std\n",
    "print('Annualized Strategy Sharpe Ratio with 50% weight of Random Forest an 50% weight of Gradient Boosting: ',rf50gb50_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different combinations of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 20\n",
    "results = []\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    weight_rf = np.random.rand()  \n",
    "    weight_gb = 1 - weight_rf  \n",
    "\n",
    "    SPY['combination'] = (SPY['strategy_rf'] * weight_rf +\n",
    "                          SPY['strategy_gb'] * weight_gb)\n",
    "\n",
    "    diffs = (SPY['combination'] - 1).diff()\n",
    "    annual_returns = diffs.mean()\n",
    "    annual_std = diffs.std()\n",
    "    sharpe_ratio = annual_returns / annual_std\n",
    "\n",
    "    results.append(((weight_rf, weight_gb), sharpe_ratio))\n",
    "\n",
    "for i, (combination, sharpe_ratio) in enumerate(results): \n",
    "        print(f\"Weights (RF, GB): {combination}, Sharpe Ratio: {sharpe_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best Sharpe Ratio among all simulations\n",
    "best_combination, best_sharpe_ratio = max(results, key=lambda x: x[1])\n",
    "print(f'Best Combination Weights (RF, GB): {best_combination}')\n",
    "print(f'Best Sharpe Ratio: {best_sharpe_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
